---
layout: post
title: Why Dashboards Donâ€™t Get Used â€” and What We Can Do About It
tags: [Career]
comments: true
---

Why Dashboards Donâ€™t Get Used â€” and What We Can Do About It

After a few years working as a data analyst in consulting, I started noticing a pattern:
Even well-designed dashboards sometimes go completely unused.
At first, I thought: maybe the analysis wasnâ€™t helpful enough. But the more I worked across projects, the clearer it became â€” the gap wasnâ€™t just technical. It was often in expectations, communication, and context.

Here are the top three reasons Iâ€™ve seen why dashboards get ignored, and how Iâ€™ve learned to respond:

1. Data quality issues: business stops trusting the numbers
If a dashboard is out-of-date, misaligned with business understanding, or inconsistent with other data sources, stakeholders will simply stop using it.

This is often not the analystâ€™s fault â€” the root causes could be upstream data issues, broken pipelines, or inconsistent input logic.
But from the stakeholderâ€™s perspective, â€œthe dashboard is wrong.â€

ğŸ“Œ What I do:

Set up a transparent issue log and keep stakeholders updated on progress and timelines;

Offer simple explanations of how the data flows â€” especially for system-level issues;

If the root cause is manual input or operational inconsistencies, I support teams in defining input standards and light data validation practices.

Data analysts arenâ€™t here to take the blame â€” but we can help everyone understand what went wrong and how weâ€™ll fix it.

2. The output doesnâ€™t answer the real business question
A common case:
Business wants to know why users are churning.
The analyst returns a breakdown of churn rates. Technically complete â€” but business still doesn't know what to do next.

ğŸ“Œ One example from my work: In an international payments project, I noticed that each day a large number of users were interacting with the auto-debit (direct debit) feature â€” almost the same volume as new users. This hinted at stagnation in net user growth.

By linking together behavioral data (subscription sign-ups, cancellations, debit attempts, customer service interactions), and supporting it with survey responses, we discovered:

Many users authorized auto-debit when signing up (e.g., for video subscriptions),

But forgot about it, and when they saw a future charge, couldnâ€™t trace where it came from,

This created a sense of insecurity, leading them to cancel the feature.

We recommended:

Adding payment reminders before deductions,

Making billing pages clearer with more specific charge details.

In this case, the value came not from showing what happened, but explaining why â€” and giving a solution that could actually be implemented.

3. The presentation is too technical or too overwhelming
Terms like p-values, confidence intervals, or overly dense visuals (like Sankey diagrams or complex heatmaps) can easily confuse non-technical stakeholders.

ğŸ“Œ How I approach this:

Keep the language clean and avoid jargon when possible;

Show key takeaways before visuals, not after;

Ask myself: â€œIf I were in the business team, would I know what to do after seeing this slide?â€

âœ¨ In short:
Sometimes dashboards fail not because the analysis is wrong â€” but because the gap between data and business action is still too wide.

As data analysts, our role isnâ€™t just to produce charts.
Itâ€™s to build understanding, trust, and ultimately â€” action.
